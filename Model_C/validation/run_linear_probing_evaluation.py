import torch
import torch.nn as nn
import pandas as pd
import numpy as np
import os
import random
from collections import OrderedDict
from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
from sklearn.manifold import TSNE
from sklearn.decomposition import PCA
from sklearn.metrics.pairwise import cosine_similarity
import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')

# =============================================================================
# éšæœºç§å­è®¾ç½®
# =============================================================================

def set_random_seed(seed=42):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

set_random_seed(42)

# =============================================================================
# é…ç½®å‚æ•° (Configuration Parameters)
# =============================================================================

LABEL_PREPROCESSING_METHOD = 'robust_per_feature'
EMBEDDING_PREPROCESSING_METHOD = 'none'  # é»˜è®¤ä¸ç¼©æ”¾åµŒå…¥å‘é‡ï¼ˆä¿ç•™åŸå§‹è¯­ä¹‰ï¼‰

# =============================================================================
# æ ‡ç­¾é¢„å¤„ç†
# =============================================================================

def preprocess_labels(labels_np, method='robust_per_feature', verbose=True):
    n_samples, n_features = labels_np.shape

    if method == 'global_standard':
        scaler = StandardScaler()
        labels_scaled = scaler.fit_transform(labels_np)
    elif method in ['per_feature_standard', 'per_feature_minmax', 'per_feature_robust', 'robust_per_feature']:
        labels_scaled = np.zeros_like(labels_np)
        for i in range(n_features):
            feature_data = labels_np[:, i].reshape(-1, 1)
            if method == 'per_feature_standard':
                feature_scaler = StandardScaler()
            elif method == 'per_feature_minmax':
                feature_scaler = MinMaxScaler()
            elif method in ['per_feature_robust', 'robust_per_feature']:
                feature_scaler = RobustScaler()
            labels_scaled[:, i] = feature_scaler.fit_transform(feature_data).flatten()

        class MultiFeatureScaler:
            def __init__(self, scalers, method):
                self.scalers = scalers
                self.method = method

            def transform(self, data):
                result = np.zeros_like(data)
                n_features = data.shape[1]
                for i in range(n_features):
                    feature_data = data[:, i].reshape(-1, 1)
                    result[:, i] = self.scalers[i].transform(feature_data).flatten()
                return result

            def inverse_transform(self, data):
                result = np.zeros_like(data)
                for i in range(n_features):
                    feature_data = data[:, i].reshape(-1, 1)
                    result[:, i] = self.scalers[i].inverse_transform(feature_data).flatten()
                return result

        scalers = []
        for i in range(n_features):
            feature_data = labels_np[:, i].reshape(-1, 1)
            if method == 'per_feature_standard':
                scalers.append(StandardScaler().fit(feature_data))
            elif method == 'per_feature_minmax':
                scalers.append(MinMaxScaler().fit(feature_data))
            elif method in ['per_feature_robust', 'robust_per_feature']:
                scalers.append(RobustScaler().fit(feature_data))

        scaler = MultiFeatureScaler(scalers, method)
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„é¢„å¤„ç†æ–¹æ³•: {method}")

    if verbose:
        print(f"\nğŸ”§ ä½¿ç”¨æ ‡ç­¾é¢„å¤„ç†æ–¹æ³•: {method}")
        print(f"   åŸå§‹æ ‡ç­¾ç»Ÿè®¡:")
        for i in range(n_features):
            print(f"   ç‰¹å¾{i}: èŒƒå›´=[{labels_np[:, i].min():.3f}, {labels_np[:, i].max():.3f}], "
                  f"å‡å€¼={labels_np[:, i].mean():.3f}, æ–¹å·®={labels_np[:, i].var():.3f}")

        print(f"   æ ‡å‡†åŒ–åæ ‡ç­¾ç»Ÿè®¡:")
        for i in range(n_features):
            print(f"   ç‰¹å¾{i}: èŒƒå›´=[{labels_scaled[:, i].min():.3f}, {labels_scaled[:, i].max():.3f}], "
                  f"å‡å€¼={labels_scaled[:, i].mean():.3f}, æ–¹å·®={labels_scaled[:, i].var():.3f}")

    return labels_scaled, scaler

# =============================================================================
# åµŒå…¥å‘é‡é¢„å¤„ç†
# =============================================================================

def preprocess_embeddings(embeddings_np, method='none', verbose=True):
    if method == 'none':
        if verbose:
            print(f"\nğŸ”§ åµŒå…¥å‘é‡é¢„å¤„ç†: {method} (ä¸è¿›è¡Œé¢„å¤„ç†)")
        return embeddings_np, None

    n_samples, n_features = embeddings_np.shape

    if method == 'standard':
        scaler = StandardScaler()
    elif method == 'minmax':
        scaler = MinMaxScaler()
    elif method == 'robust':
        scaler = RobustScaler()
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„é¢„å¤„ç†æ–¹æ³•: {method}")

    embeddings_scaled = scaler.fit_transform(embeddings_np)

    if verbose:
        print(f"\nğŸ”§ åµŒå…¥å‘é‡é¢„å¤„ç†æ–¹æ³•: {method}")
        print(f"   å…¨å±€èŒƒå›´: [{embeddings_np.min():.6f}, {embeddings_np.max():.6f}]")
        print(f"   å…¨å±€å‡å€¼: {embeddings_np.mean():.6f} Â± {embeddings_np.std():.6f}")
        print(f"   ç»´åº¦å‡å€¼èŒƒå›´: [{embeddings_np.mean(axis=0).min():.6f}, {embeddings_np.mean(axis=0).max():.6f}]")

    return embeddings_scaled, scaler

# =============================================================================
# æ¨¡å‹å®šä¹‰
# =============================================================================

class LinearProbe(nn.Module):
    def __init__(self, in_features, out_features):
        super(LinearProbe, self).__init__()
        self.probe = nn.Linear(in_features, out_features)
    def forward(self, x):
        return self.probe(x)

class MLPProbe(nn.Module):
    def __init__(self, in_features, out_features, hidden_dim=32):  # å‡å°å®¹é‡é˜²æ­¢è¿‡æ‹Ÿåˆ
        super(MLPProbe, self).__init__()
        self.network = nn.Sequential(
            nn.Linear(in_features, hidden_dim),
            nn.LeakyReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.LeakyReLU(),
            nn.Linear(hidden_dim, out_features)
        )
        # åˆå§‹åŒ–æƒé‡
        for layer in self.network:
            if isinstance(layer, nn.Linear):
                nn.init.kaiming_normal_(layer.weight)
                
    def forward(self, x):
        return self.network(x)

# =============================================================================
# è®­ç»ƒå‡½æ•°
# =============================================================================

def train_model(embeddings, labels, model, epochs=1000, learning_rate=0.01, patience=20):
    criterion = nn.MSELoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)

    best_loss = float('inf')
    patience_counter = 0
    best_model_state = None

    for epoch in range(epochs):
        model.train()
        predictions = model(embeddings)
        loss = criterion(predictions, labels)

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        if loss.item() < best_loss:
            best_loss = loss.item()
            patience_counter = 0
            best_model_state = {k: v.clone() for k, v in model.state_dict().items()}
        else:
            patience_counter += 1

        if patience_counter >= patience:
            model.load_state_dict(best_model_state)
            break

        if (epoch + 1) % 200 == 0:
            pass  # ä¸æ‰“å°é¿å…å¹²æ‰°å±•ç¤º

    final_loss = best_loss if best_loss != float('inf') else loss.item()
    return model, final_loss

def evaluate_model(model, embeddings, labels, label_scaler=None):
    model.eval()
    with torch.no_grad():
        predictions = model(embeddings)

        # å­˜å‚¨æ ‡å‡†åŒ–å°ºåº¦çš„é¢„æµ‹ï¼Œä»¥å¤‡ä¸æ—¶ä¹‹éœ€
        predictions_scaled = predictions.clone()

        # å¦‚æœæä¾›äº†ç¼©æ”¾å™¨ï¼Œåˆ™å°†é¢„æµ‹å’Œæ ‡ç­¾éƒ½é€†å˜æ¢å›åŸå§‹å°ºåº¦è¿›è¡Œè¯„ä¼°
        if label_scaler is not None:
            predictions_np = label_scaler.inverse_transform(predictions.numpy())
            predictions = torch.tensor(predictions_np, dtype=torch.float32)
            # åŒæ—¶å°†çœŸå®æ ‡ç­¾ä¹Ÿé€†å˜æ¢å›åŸå§‹å°ºåº¦
            labels_np = label_scaler.inverse_transform(labels.numpy())
            labels = torch.tensor(labels_np, dtype=torch.float32)

        # æ— è®ºæ˜¯å¦ç¼©æ”¾ï¼Œmse_losséƒ½æ˜¯åœ¨ç›¸åŒå°ºåº¦ä¸Šè®¡ç®—çš„
        mse_loss = nn.MSELoss()(predictions, labels).item()

        # è¿”å›ï¼š
        # 1. åŸå§‹å°ºåº¦çš„é¢„æµ‹ (ç”¨äºè®¡ç®—MAE, RMSEç­‰)
        # 2. åŸå§‹å°ºåº¦ä¸Šçš„MSEæŸå¤±
        # 3. æ ‡å‡†åŒ–å°ºåº¦çš„é¢„æµ‹ (æœªä½¿ç”¨ï¼Œä½†ä¿ç•™æ¥å£)
        # 4. åŸå§‹å°ºåº¦çš„çœŸå®æ ‡ç­¾ (è¿™æ˜¯ä¿®å¤çš„å…³é”®)
        return predictions, mse_loss, predictions_scaled, labels

# =============================================================================
# æ–°å¢ï¼šç•™ä¸€æ³•äº¤å‰éªŒè¯ (Leave-One-Out Cross-Validation)
# =============================================================================

def evaluate_probe_loo_cv(embeddings, raw_labels_np, model_class, material_names, prop_names, save_path=None, **model_kwargs):
    """
    ç•™ä¸€æ³•äº¤å‰éªŒè¯ï¼šæ¯æ¬¡ç•™å‡ºä¸€ä¸ªæ ·æœ¬ä½œä¸ºæµ‹è¯•ï¼Œç”¨å…¶ä½™æ ·æœ¬è®­ç»ƒ
    è¿”å›æ¯ä¸ªæ ·æœ¬çš„é¢„æµ‹ç»“æœå’Œæ•´ä½“æ€§èƒ½æŒ‡æ ‡
    """
    print(f"\n--- ç•™ä¸€æ³•äº¤å‰éªŒè¯ {model_class.__name__} ({len(material_names)} æŠ˜) ---")

    n_samples = len(material_names)
    n_props = raw_labels_np.shape[1]

    # å­˜å‚¨æ¯æ¬¡çš„é¢„æµ‹ç»“æœ
    all_predictions = []
    all_true_labels = []
    fold_results = []

    for i in range(n_samples):
        # åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†ç´¢å¼•
        test_indices = [i]
        train_indices = [j for j in range(n_samples) if j != i]

        # å‡†å¤‡æ•°æ®
        X_train = embeddings[train_indices]
        y_train_raw = raw_labels_np[train_indices]
        X_test = embeddings[test_indices]
        y_test_raw = raw_labels_np[test_indices]
        
        test_material = material_names[i]

        # æ ¸å¿ƒä¿®å¤ï¼šåœ¨äº¤å‰éªŒè¯å¾ªç¯å†…éƒ¨è¿›è¡Œæ•°æ®ç¼©æ”¾
        # 1. ä»…åœ¨å½“å‰æŠ˜å çš„è®­ç»ƒé›†ä¸Šæ‹Ÿåˆç¼©æ”¾å™¨
        y_train_scaled_np, fold_scaler = preprocess_labels(
            y_train_raw, method=LABEL_PREPROCESSING_METHOD, verbose=False
        )
        
        # 2. ä½¿ç”¨ä»è®­ç»ƒé›†å­¦ä¹ åˆ°çš„ç¼©æ”¾å™¨è½¬æ¢æµ‹è¯•é›†
        y_test_scaled_np = fold_scaler.transform(y_test_raw)

        # è½¬æ¢ä¸º PyTorch Tensors
        y_train = torch.tensor(y_train_scaled_np, dtype=torch.float32)
        y_test = torch.tensor(y_test_scaled_np, dtype=torch.float32)

        # è®­ç»ƒæ¨¡å‹
        model = model_class(**model_kwargs)
        trained_model, train_loss = train_model(X_train, y_train, model, epochs=1000, patience=20)

        # åœ¨æµ‹è¯•æ ·æœ¬ä¸Šé¢„æµ‹ï¼Œå¹¶ä½¿ç”¨å½“å‰æŠ˜å çš„ç¼©æ”¾å™¨è¿›è¡Œé€†è½¬æ¢
        predictions, test_loss, _, true_labels = evaluate_model(trained_model, X_test, y_test, fold_scaler)

        # å­˜å‚¨ç»“æœ (predictions å’Œ true_labels å‡ä¸ºåŸå§‹å°ºåº¦)
        pred_np = predictions.numpy()
        true_np = true_labels.numpy()

        all_predictions.append(pred_np[0])  # å•ä¸ªæ ·æœ¬é¢„æµ‹
        all_true_labels.append(true_np[0])  # å•ä¸ªæ ·æœ¬çœŸå®å€¼

        fold_result = {
            'fold': i+1,
            'test_material': test_material,
            'train_loss': train_loss,
            'test_loss': test_loss,
            'predictions': pred_np[0],
            'true_labels': true_np[0]
        }
        fold_results.append(fold_result)

        print(f"  æŠ˜ {i+1:2d}: {test_material:<12} â†’ MSE: {test_loss:.6f}")

    # è®¡ç®—æ•´ä½“æ€§èƒ½æŒ‡æ ‡
    all_predictions = np.array(all_predictions)
    all_true_labels = np.array(all_true_labels)

    # ä¸ºæ¯ä¸ªå±æ€§è®¡ç®—æŒ‡æ ‡
    prop_metrics = {}
    overall_metrics = {'mae': [], 'rmse': [], 'r2': []}

    for prop_idx in range(n_props):
        prop_name = prop_names[prop_idx] if prop_idx < len(prop_names) else f"Prop_{prop_idx}"
        true_vals = all_true_labels[:, prop_idx]
        pred_vals = all_predictions[:, prop_idx]

        # è®¡ç®—MAE, RMSE, RÂ²
        mae = np.mean(np.abs(pred_vals - true_vals))
        rmse = np.sqrt(np.mean((pred_vals - true_vals)**2))
        ss_res = np.sum((true_vals - pred_vals)**2)
        ss_tot = np.sum((true_vals - np.mean(true_vals))**2)
        r2 = 1 - (ss_res / ss_tot) if ss_tot != 0 else 0

        prop_metrics[prop_name] = {
            'mae': mae,
            'rmse': rmse,
            'r2': r2
        }

        overall_metrics['mae'].append(mae)
        overall_metrics['rmse'].append(rmse)
        overall_metrics['r2'].append(r2)

    # è®¡ç®—å¹³å‡æŒ‡æ ‡
    avg_metrics = {
        'mae_avg': np.mean(overall_metrics['mae']),
        'rmse_avg': np.mean(overall_metrics['rmse']),
        'r2_avg': np.mean(overall_metrics['r2'])
    }

    # åˆ›å»ºè¯¦ç»†ç»“æœè¡¨æ ¼
    loo_results = []
    for i, material in enumerate(material_names):
        row = {'Material': material}
        for prop_idx, prop_name in enumerate(prop_names):
            row[f'True_{prop_name}'] = all_true_labels[i, prop_idx]
            row[f'Pred_{prop_name}'] = all_predictions[i, prop_idx]
        loo_results.append(row)

    loo_table = pd.DataFrame(loo_results)

    # æ‰“å°ç»“æœ
    print(f"\nğŸ“Š ç•™ä¸€æ³•äº¤å‰éªŒè¯ç»“æœ:")
    print(f"   å¹³å‡MAE: {avg_metrics['mae_avg']:.4f}")
    print(f"   å¹³å‡RMSE: {avg_metrics['rmse_avg']:.4f}")
    print(f"   å¹³å‡RÂ²: {avg_metrics['r2_avg']:.4f}")
    for prop_name, metrics in prop_metrics.items():
        print(f"   {prop_name}: MAE={metrics['mae']:.4f}, RMSE={metrics['rmse']:.4f}, RÂ²={metrics['r2']:.4f}")

    print(f"\nğŸ” é¢„æµ‹ vs çœŸå®å€¼è¯¦ç»†å¯¹æ¯”:")
    print(loo_table.round(4))

    # ä¿å­˜ç»“æœ
    if save_path:
        os.makedirs(os.path.dirname(save_path), exist_ok=True)
        loo_table.to_csv(save_path, index=False, float_format='%.6f')
        print(f"   ğŸ’¾ LOO-CV è¯¦ç»†ç»“æœå·²ä¿å­˜: {save_path}")

        # ä¿å­˜åŒ…å«æ€§èƒ½æŒ‡æ ‡çš„æ‰©å±•ç‰ˆæœ¬
        extended_data = loo_table.copy()
        for prop_name in prop_names:
            true_col = f'True_{prop_name}'
            pred_col = f'Pred_{prop_name}'
            extended_data[f'Error_{prop_name}'] = extended_data[pred_col] - extended_data[true_col]
            extended_data[f'Abs_Error_{prop_name}'] = np.abs(extended_data[pred_col] - extended_data[true_col])
            extended_data[f'Rel_Error_{prop_name}'] = (extended_data[pred_col] - extended_data[true_col]) / extended_data[true_col] * 100

        # æ·»åŠ æ€§èƒ½æŒ‡æ ‡åˆ°æ‰©å±•æ•°æ®
        metrics_summary = {}
        for prop_name in prop_names:
            metrics_summary[f'MAE_{prop_name}'] = prop_metrics[prop_name]['mae']
            metrics_summary[f'RMSE_{prop_name}'] = prop_metrics[prop_name]['rmse']
            metrics_summary[f'R2_{prop_name}'] = prop_metrics[prop_name]['r2']

        # åˆ›å»ºæ€§èƒ½æŒ‡æ ‡è¡Œ
        metrics_row = {'Material': 'METRICS'}
        for prop_name in prop_names:
            metrics_row.update({
                f'True_{prop_name}': metrics_summary[f'MAE_{prop_name}'],
                f'Pred_{prop_name}': metrics_summary[f'RMSE_{prop_name}'],
                f'Error_{prop_name}': metrics_summary[f'R2_{prop_name}'],
                f'Abs_Error_{prop_name}': np.nan,
                f'Rel_Error_{prop_name}': np.nan
            })

        extended_data = pd.concat([extended_data, pd.DataFrame([metrics_row])], ignore_index=True)
        extended_save_path = save_path.replace('.csv', '_detailed.csv')
        extended_data.to_csv(extended_save_path, index=False, float_format='%.6f')
        print(f"   ğŸ’¾ LOO-CV æ‰©å±•ç»“æœ(åŒ…å«è¯¯å·®åˆ†æ)å·²ä¿å­˜: {extended_save_path}")

    return avg_metrics, prop_metrics, loo_table, fold_results

# =============================================================================
# æ–°å¢ï¼šå…¨ä½“æ•°æ®è®­ç»ƒ + ç”Ÿæˆé¢„æµ‹å¯¹æ¯”è¡¨æ ¼
# =============================================================================

def evaluate_probe_on_all_data(embeddings, raw_labels_np, model_class, material_names, prop_names, save_path=None, **model_kwargs):
    print(f"\n--- Training {model_class.__name__} on all {len(material_names)} samples ---")

    # 1. åœ¨å‡½æ•°å†…éƒ¨è¿›è¡Œæ•°æ®ç¼©æ”¾
    labels_scaled_np, label_scaler = preprocess_labels(
        raw_labels_np, method=LABEL_PREPROCESSING_METHOD, verbose=False
    )
    labels_scaled_torch = torch.tensor(labels_scaled_np, dtype=torch.float32)

    # 2. è®­ç»ƒæ¨¡å‹
    model = model_class(**model_kwargs)
    trained_model, train_loss = train_model(embeddings, labels_scaled_torch, model, epochs=1000, patience=20)
    
    # 3. è¯„ä¼°æ¨¡å‹ï¼Œå¹¶ä½¿ç”¨ç¼©æ”¾å™¨é€†è½¬æ¢
    predictions, test_loss, _, _ = evaluate_model(trained_model, embeddings, labels_scaled_torch, label_scaler)

    # åˆ›å»ºé¢„æµ‹å¯¹æ¯”è¡¨
    data = {'Material': material_names}
    n_props = raw_labels_np.shape[1]
    pred_np = predictions.numpy()
    
    # è·å–åŸå§‹å°ºåº¦çš„çœŸå®æ ‡ç­¾ç”¨äºå¯¹æ¯”
    true_np = raw_labels_np

    for i in range(n_props):
        prop_name = prop_names[i] if i < len(prop_names) else f"Prop_{i}"
        data[f'True_{prop_name}'] = true_np[:, i]
        data[f'Pred_{prop_name}'] = pred_np[:, i]

    pred_table = pd.DataFrame(data)

    print("\nğŸ“Š é¢„æµ‹ vs çœŸå®å€¼å¯¹æ¯”è¡¨:")
    print(pred_table.round(3))

    # ä¿å­˜è¯¦ç»†é¢„æµ‹å¯¹æ¯”æ•°æ®åˆ°CSVæ–‡ä»¶
    if save_path:
        os.makedirs(os.path.dirname(save_path), exist_ok=True)
        pred_table.to_csv(save_path, index=False, float_format='%.6f')
        print(f"   ğŸ’¾ è¯¦ç»†é¢„æµ‹æ•°æ®å·²ä¿å­˜: {save_path}")

        # åŒæ—¶ä¿å­˜ä¸€ä¸ªåŒ…å«è¯¯å·®åˆ†æçš„æ‰©å±•ç‰ˆæœ¬
        extended_data = pred_table.copy()
        for i in range(n_props):
            prop_name = prop_names[i] if i < len(prop_names) else f"Prop_{i}"
            true_col = f'True_{prop_name}'
            pred_col = f'Pred_{prop_name}'
            extended_data[f'Error_{prop_name}'] = extended_data[pred_col] - extended_data[true_col]
            extended_data[f'Abs_Error_{prop_name}'] = np.abs(extended_data[pred_col] - extended_data[true_col])
            extended_data[f'Rel_Error_{prop_name}'] = (extended_data[pred_col] - extended_data[true_col]) / extended_data[true_col] * 100

        extended_save_path = save_path.replace('.csv', '_detailed.csv')
        extended_data.to_csv(extended_save_path, index=False, float_format='%.6f')
        print(f"   ğŸ’¾ æ‰©å±•é¢„æµ‹æ•°æ®(åŒ…å«è¯¯å·®åˆ†æ)å·²ä¿å­˜: {extended_save_path}")

    return test_loss, pred_table, trained_model

# =============================================================================
# æ–°å¢ï¼šä½™å¼¦ç›¸ä¼¼åº¦çƒ­åŠ›å›¾
# =============================================================================

def plot_cosine_similarity_heatmap(embeddings, material_names, save_path=None):
    sim_matrix = cosine_similarity(embeddings)
    plt.figure(figsize=(7, 6))
    sns.set(font_scale=1.0)
    sns.heatmap(sim_matrix, annot=True, fmt=".2f", xticklabels=material_names, yticklabels=material_names,
                cmap='coolwarm', center=0, square=True, cbar_kws={'shrink': 0.8})
    plt.title('Cosine Similarity between Material Embeddings', fontsize=14, pad=20)
    plt.xticks(rotation=45, ha='right')
    plt.yticks(rotation=0)
    plt.tight_layout()
    if save_path:
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        print(f"   âœ… Cosine similarity heatmap saved: {save_path}")
    plt.close()

# =============================================================================
# æ–°å¢ï¼šä¸ºLOO-CVç»“æœç»˜åˆ¶é¢„æµ‹-çœŸå®å€¼æ•£ç‚¹å›¾
# =============================================================================

def plot_loo_predictions(loo_table, prop_names, model_name, probe_type, save_dir):
    """ä¸ºç•™ä¸€æ³•äº¤å‰éªŒè¯ç»“æœç”Ÿæˆé¢„æµ‹å€¼ vs. çœŸå®å€¼æ•£ç‚¹å›¾"""
    print(f"\n--- æ­£åœ¨ä¸º {model_name} ({probe_type}, LOO-CV) ç”Ÿæˆé¢„æµ‹-çœŸå®å€¼æ•£ç‚¹å›¾ ---")
    n_props = len(prop_names)
    n_cols = 2
    n_rows = (n_props + 1) // n_cols
    
    fig, axes = plt.subplots(n_rows, n_cols, figsize=(6 * n_cols, 5 * n_rows))
    axes = axes.flatten()

    for i, prop_name in enumerate(prop_names):
        ax = axes[i]
        true_col = f'True_{prop_name}'
        pred_col = f'Pred_{prop_name}'
        
        true_vals = loo_table[true_col].values
        pred_vals = loo_table[pred_col].values
        
        # æ–°å¢ï¼šè®¡ç®— RÂ² åˆ†æ•°
        ss_res = np.sum((true_vals - pred_vals)**2)
        ss_tot = np.sum((true_vals - np.mean(true_vals))**2)
        r2 = 1 - (ss_res / ss_tot) if ss_tot > 1e-9 else 0.0

        ax.scatter(true_vals, pred_vals, s=150, alpha=0.8, edgecolors='k', linewidth=0.5, label='Predictions')
        
        # ç»˜åˆ¶ y=x å¯¹è§’çº¿
        all_vals = np.concatenate([true_vals, pred_vals])
        lims = [np.min(all_vals), np.max(all_vals)]
        margin = (lims[1] - lims[0]) * 0.1 if (lims[1] - lims[0]) > 1e-6 else 0.1
        lims = [lims[0] - margin, lims[1] + margin]
        
        ax.plot(lims, lims, 'k--', alpha=0.75, zorder=0, label='y=x (Ideal)')
        ax.set_xlim(lims)
        ax.set_ylim(lims)
        
        ax.set_xlabel(f'True {prop_name}', fontsize=12)
        ax.set_ylabel(f'Predicted {prop_name}', fontsize=12)
        ax.set_title(f'{prop_name}\n$R^2 = {r2:.3f}$', fontsize=14)
        ax.grid(True, alpha=0.3)
        ax.legend()

    # éšè—å¤šä½™çš„å­å›¾
    for i in range(n_props, len(axes)):
        axes[i].set_visible(False)

    fig.suptitle(f'LOO-CV: Predicted vs. True Values ({model_name} - {probe_type})', fontsize=16, y=1.02)
    plt.tight_layout()
    
    save_path = os.path.join(save_dir, f"{model_name}_{probe_type.lower()}_loo_cv_predictions_scatter.png")
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    print(f"   âœ… LOO-CV æ•£ç‚¹å›¾å·²ä¿å­˜: {save_path}")
    plt.close()

# =============================================================================
# æ–°å¢æ¨¡å— 1.1: æ®‹å·®åˆ†å¸ƒç›´æ–¹å›¾/æ ¸å¯†åº¦å›¾
# =============================================================================

def plot_residual_distribution(linear_loo_table, mlp_loo_table, prop_names, model_name, save_dir):
    """ä¸ºLOO-CVç»“æœç”Ÿæˆæ®‹å·®çš„æç´å›¾ï¼Œä»¥å±•ç¤ºå…¶åˆ†å¸ƒ"""
    print(f"\n--- æ­£åœ¨ä¸º {model_name} ç”Ÿæˆæ®‹å·®åˆ†å¸ƒå›¾ ---")
    
    residuals_data = []
    for prop in prop_names:
        true_col = f'True_{prop}'
        # Linear Probe residuals
        pred_col_linear = f'Pred_{prop}'
        res_linear = linear_loo_table[pred_col_linear] - linear_loo_table[true_col]
        for r in res_linear:
            residuals_data.append({'Property': prop, 'Residual': r, 'Probe': 'Linear'})
        # MLP Probe residuals
        pred_col_mlp = f'Pred_{prop}'
        res_mlp = mlp_loo_table[pred_col_mlp] - mlp_loo_table[true_col]
        for r in res_mlp:
            residuals_data.append({'Property': prop, 'Residual': r, 'Probe': 'MLP'})

    df_res = pd.DataFrame(residuals_data)

    plt.figure(figsize=(15, 8))
    sns.violinplot(data=df_res, x='Property', y='Residual', hue='Probe', split=True, inner="quart", fill=False)
    
    plt.axhline(0, color='r', linestyle='--', label='Zero Error')
    plt.title(f'Distribution of LOO-CV Residuals for {model_name}', fontsize=16)
    plt.xticks(rotation=15, ha='right')
    plt.ylabel('Residual (Predicted - True)', fontsize=12)
    plt.xlabel('')
    plt.legend(title='Probe Type')
    plt.grid(axis='y', linestyle='--', alpha=0.7)
    plt.tight_layout()

    save_path = os.path.join(save_dir, f"{model_name}_residuals_distribution.png")
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    print(f"   âœ… æ®‹å·®åˆ†å¸ƒå›¾å·²ä¿å­˜: {save_path}")
    plt.close()


# =============================================================================
# æ–°å¢æ¨¡å— 1.2: ç•™ä¸€æ³•äº¤å‰éªŒè¯è¯¯å·®ç®±çº¿å›¾
# =============================================================================

def plot_loo_error_boxplot(linear_loo_table, mlp_loo_table, prop_names, model_name, save_dir):
    """ä¸ºLOO-CVç»“æœçš„ç»å¯¹è¯¯å·®ç”Ÿæˆç®±çº¿å›¾"""
    print(f"\n--- æ­£åœ¨ä¸º {model_name} ç”ŸæˆLOO-CVè¯¯å·®ç®±çº¿å›¾ ---")
    
    error_data = []
    for prop in prop_names:
        true_col = f'True_{prop}'
        # Linear Probe errors
        pred_col_linear = f'Pred_{prop}'
        abs_err_linear = np.abs(linear_loo_table[pred_col_linear] - linear_loo_table[true_col])
        for e in abs_err_linear:
            error_data.append({'Property': prop, 'Absolute Error': e, 'Probe': 'Linear'})
        # MLP Probe errors
        pred_col_mlp = f'Pred_{prop}'
        abs_err_mlp = np.abs(mlp_loo_table[pred_col_mlp] - mlp_loo_table[true_col])
        for e in abs_err_mlp:
            error_data.append({'Property': prop, 'Absolute Error': e, 'Probe': 'MLP'})

    df_err = pd.DataFrame(error_data)

    plt.figure(figsize=(15, 8))
    sns.boxplot(data=df_err, x='Property', y='Absolute Error', hue='Probe')
    
    plt.title(f'Boxplot of LOO-CV Absolute Errors for {model_name}', fontsize=16)
    plt.xticks(rotation=15, ha='right')
    plt.ylabel('Absolute Error', fontsize=12)
    plt.xlabel('')
    plt.grid(axis='y', linestyle='--', alpha=0.7)
    plt.tight_layout()

    save_path = os.path.join(save_dir, f"{model_name}_loo_error_boxplot.png")
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    print(f"   âœ… LOO-CVè¯¯å·®ç®±çº¿å›¾å·²ä¿å­˜: {save_path}")
    plt.close()


# =============================================================================
# æ–°å¢æ¨¡å— 2.1: æ ‡åº¦åç‰¹å¾ä¸¤ä¸¤æ•£ç‚¹çŸ©é˜µ
# =============================================================================

def plot_feature_pairplot(scaled_labels_np, prop_names, save_path="visualizations/feature_pairplot.png"):
    """ç»˜åˆ¶æ ‡åº¦åç›®æ ‡å±æ€§ä¹‹é—´çš„æ•£ç‚¹çŸ©é˜µï¼Œä»¥æ£€æŸ¥ç›¸å…³æ€§"""
    print(f"\n--- æ­£åœ¨ç”Ÿæˆæ ‡åº¦åç‰¹å¾çš„æ•£ç‚¹çŸ©é˜µ ---")
    df_features = pd.DataFrame(scaled_labels_np, columns=prop_names)
    
    g = sns.pairplot(df_features, corner=True, diag_kind='kde')
    g.fig.suptitle('Pairplot of Scaled Target Properties', y=1.02, fontsize=16)
    
    os.makedirs(os.path.dirname(save_path), exist_ok=True)
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    print(f"   âœ… ç‰¹å¾æ•£ç‚¹çŸ©é˜µå·²ä¿å­˜: {save_path}")
    plt.close()


# =============================================================================
# æ–°å¢æ¨¡å— 2.2: åµŒå…¥ç»Ÿè®¡æŸ±çŠ¶å›¾
# =============================================================================

def plot_embedding_statistics(embeddings_np, material_names, model_name, save_dir):
    """ä¸ºæ¯ä¸ªææ–™çš„åµŒå…¥å‘é‡ç»˜åˆ¶å…³é”®ç»Ÿè®¡é‡çš„çƒ­åŠ›å›¾"""
    print(f"\n--- æ­£åœ¨ä¸º {model_name} ç”ŸæˆåµŒå…¥ç»Ÿè®¡çƒ­åŠ›å›¾ ---")
    
    stats_data = {
        'Mean': np.mean(embeddings_np, axis=1),
        'Std': np.std(embeddings_np, axis=1),
        'Min': np.min(embeddings_np, axis=1),
        'Max': np.max(embeddings_np, axis=1)
    }
    df_stats = pd.DataFrame(stats_data, index=material_names)

    plt.figure(figsize=(10, 8))
    sns.heatmap(df_stats, annot=True, fmt=".3f", cmap='viridis', linewidths=.5, cbar_kws={'label': 'Embedding Value'})
    
    plt.title(f'Embedding Stats by Material for {model_name}', fontsize=16, pad=20)
    plt.xlabel('Statistics', fontsize=12)
    plt.ylabel('Material', fontsize=12)
    plt.xticks(rotation=0)
    plt.yticks(rotation=0)
    plt.tight_layout()

    save_path = os.path.join(save_dir, f"{model_name}_embedding_statistics.png")
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    print(f"   âœ… åµŒå…¥ç»Ÿè®¡çƒ­åŠ›å›¾å·²ä¿å­˜: {save_path}")
    plt.close()


# =============================================================================
# æ–°å¢æ¨¡å— 3.1: æ®‹å·® vs. å…³é”®ç‰¹å¾æ•£ç‚¹å›¾
# =============================================================================

def plot_residuals_vs_feature(linear_loo_table, mlp_loo_table, scaled_labels_np, prop_names, model_name, save_dir, feature_idx=1):
    """ç»˜åˆ¶æ®‹å·®ä¸ä¸€ä¸ªå…³é”®ç‰¹å¾çš„æ•£ç‚¹å›¾ï¼Œä»¥æ£€æŸ¥ç³»ç»Ÿæ€§åå·®"""
    key_feature_name = prop_names[feature_idx]
    print(f"\n--- æ­£åœ¨ä¸º {model_name} ç”Ÿæˆæ®‹å·® vs. {key_feature_name} æ•£ç‚¹å›¾ ---")
    key_feature_values = scaled_labels_np[:, feature_idx]

    n_props = len(prop_names)
    fig, axes = plt.subplots(n_props, 2, figsize=(15, 5 * n_props), sharex=True)

    for i, prop in enumerate(prop_names):
        # Linear Probe
        true_col = f'True_{prop}'
        pred_col_linear = f'Pred_{prop}'
        res_linear = linear_loo_table[pred_col_linear] - linear_loo_table[true_col]
        
        ax_linear = axes[i, 0]
        sns.scatterplot(x=key_feature_values, y=res_linear, ax=ax_linear)
        ax_linear.axhline(0, color='r', linestyle='--')
        ax_linear.set_title(f'Linear Probe: {prop} Residuals', fontsize=12)
        ax_linear.set_ylabel('Residual')

        # MLP Probe
        pred_col_mlp = f'Pred_{prop}'
        res_mlp = mlp_loo_table[pred_col_mlp] - mlp_loo_table[true_col]
        
        ax_mlp = axes[i, 1]
        sns.scatterplot(x=key_feature_values, y=res_mlp, ax=ax_mlp)
        ax_mlp.axhline(0, color='r', linestyle='--')
        ax_mlp.set_title(f'MLP Probe: {prop} Residuals', fontsize=12)
        ax_mlp.set_ylabel('')

    axes[-1, 0].set_xlabel(f'Scaled {key_feature_name}', fontsize=12)
    axes[-1, 1].set_xlabel(f'Scaled {key_feature_name}', fontsize=12)
    
    fig.suptitle(f'Residuals vs. {key_feature_name} for {model_name}', fontsize=16, y=1.01)
    plt.tight_layout()

    save_path = os.path.join(save_dir, f"{model_name}_residuals_vs_feature.png")
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    print(f"   âœ… æ®‹å·®-ç‰¹å¾æ•£ç‚¹å›¾å·²ä¿å­˜: {save_path}")
    plt.close()


# =============================================================================
# å¯è§†åŒ–åµŒå…¥ç©ºé—´
# =============================================================================

def visualize_embeddings(embeddings, material_names, method='tsne', save_path=None):
    print(f"\n--- æ–¹æ¡ˆ: åµŒå…¥å¯è§†åŒ– ({method.upper()}) ---")

    if method == 'tsne':
        reducer = TSNE(n_components=2, random_state=42, perplexity=min(5, len(embeddings)-1))
    else:
        reducer = PCA(n_components=2, random_state=42)

    embeddings_2d = reducer.fit_transform(embeddings)
    plt.figure(figsize=(8, 6))

    unique_materials = list(set(material_names))
    colors = plt.cm.tab10(np.linspace(0, 1, len(unique_materials)))

    for i, material in enumerate(unique_materials):
        mask = [name == material for name in material_names]
        plt.scatter(embeddings_2d[mask, 0], embeddings_2d[mask, 1],
                   color=colors[i], label=material, s=150, alpha=0.8, edgecolors='k', linewidth=0.5)

    plt.title(f'Material Embedding Space ({method.upper()})', fontsize=14)
    plt.xlabel(f'{method.upper()} Component 1', fontsize=12)
    plt.ylabel(f'{method.upper()} Component 2', fontsize=12)
    plt.legend(title="Materials", fontsize=10, title_fontsize=11)
    plt.grid(True, alpha=0.3)
    plt.tight_layout()

    if save_path:
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        print(f"   âœ… Visualization saved: {save_path}")
    plt.close()

# =============================================================================
# èšç±»è¯„ä¼°ï¼ˆä»…ä¿ç•™è½®å»“åˆ†æ•°ä½œä¸ºå‚è€ƒï¼Œåˆ é™¤NMIï¼‰
# =============================================================================

def evaluate_clustering_metrics(embeddings, material_names, n_clusters_range=range(2, 6)):
    print("\n--- æ–¹æ¡ˆ: èšç±»ç»“æ„æ¢ç´¢ (ä»…ä¾›è§‚å¯Ÿ) ---")

    best_silhouette = -2
    best_n_clusters = 2
    try:
        for n_clusters in n_clusters_range:
            kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
            cluster_labels = kmeans.fit_predict(embeddings)

            if len(set(cluster_labels)) == n_clusters:
                silhouette_avg = silhouette_score(embeddings, cluster_labels)
                print(f"  n_clusters={n_clusters}: Silhouette={silhouette_avg:.3f}")
                if silhouette_avg > best_silhouette:
                    best_silhouette = silhouette_avg
                    best_n_clusters = n_clusters
    except Exception as e:
        print(f"  èšç±»è®¡ç®—å¼‚å¸¸: {e}")

    print(f"   æœ€ä½³èšç±»æ•°: {best_n_clusters}, æœ€ä½³è½®å»“åˆ†æ•°: {best_silhouette:.3f} (ä»…ä¾›å‚è€ƒ)")
    return {
        'best_n_clusters': best_n_clusters,
        'best_silhouette': best_silhouette,
        'note': 'å› æ ·æœ¬æå°‘ï¼Œæ­¤æŒ‡æ ‡ä»…ä¾›å‚è€ƒ'
    }

# =============================================================================
# ä¸»ç¨‹åº
# =============================================================================

def main():
    MODEL_DIRS = {
        "model-1": "model-1-09171648",
        "model-2": "model-2-09180905",
        "model-3": "model-3-09181323",
        "model-4": "model-4-09181820"
    }
    EMBEDDING_DIM = 128
    PROPERTY_NAMES = ['Formation Energy (eV)', 'Bandgap (eV)', 'Fermi Level (eV)', 'Eff. Mass (m0)']

    true_labels_placeholder = OrderedDict([
        ('CsPbCl3',    [-26.35, 3.0, 3.5038817, 0.21]),
        ('CsPbBr3',    [-23.81, 2.3, 2.964493582, 0.22]),
        ('CH3NH3GeI3', [-12.94, 1.9, 2.158559313, 0.15]),
        ('CH3NH3PbI3', [-13.06, 1.51, 1.754137302, 0.15]),
        ('CsPbI3',     [-32.8, 1.7, 3.458034525, 0.2])
    ])

    print("="*80)
    print("ğŸ”¬ å°æ ·æœ¬æ¼”ç¤ºæ¨¡å¼ â€”â€” è‡ªç›‘ç£åµŒå…¥æ¡†æ¶å¯è¡Œæ€§éªŒè¯ç³»ç»Ÿ")
    print("="*80)
    print("âš ï¸  å½“å‰æ¨¡å¼: n=5 æ ·æœ¬ï¼Œä»…ç”¨äºæ–¹æ³•æµç¨‹æ¼”ç¤ºå’Œè§†è§‰å±•ç¤º")
    print("âš ï¸  æ‰€æœ‰æ•°å€¼ç»“æœä¸å…·å¤‡ç»Ÿè®¡æ˜¾è‘—æ€§ï¼Œä½†å¯åæ˜ æ¡†æ¶è¿è¡Œèƒ½åŠ›")
    print("="*80)

    labels_np = np.array(list(true_labels_placeholder.values()))
    material_names = list(true_labels_placeholder.keys())

    # ä»…ä¸ºç»˜å›¾å’ŒæŠ¥å‘Šç”Ÿæˆä¸€æ¬¡æ ‡åº¦åçš„æ ‡ç­¾
    labels_scaled_np_for_plotting, _ = preprocess_labels(
        labels_np, method=LABEL_PREPROCESSING_METHOD, verbose=False
    )
    
    # === æ–°å¢ï¼šç»˜åˆ¶æ ‡åº¦åç‰¹å¾çš„æ•£ç‚¹çŸ©é˜µ (æ¨¡å‹æ— å…³) ===
    plot_feature_pairplot(labels_scaled_np_for_plotting, PROPERTY_NAMES)

    all_models_results = {}

    for model_name, model_dir in MODEL_DIRS.items():
        print(f"\n{'='*70}")
        print(f"ğŸ¯ å¤„ç†æ¨¡å‹: {model_name}")
        print(f"{'='*70}")

        embedding_file = os.path.join(model_dir, "embeddings", f"{model_dir}_embeddings.csv")
        if not os.path.exists(embedding_file):
            print(f"âŒ åµŒå…¥æ–‡ä»¶æœªæ‰¾åˆ°ï¼Œè·³è¿‡")
            continue

        df = pd.read_csv(embedding_file)
        if list(df['material']) != material_names:
            print("âŒ ææ–™é¡ºåºä¸åŒ¹é…ï¼Œè·³è¿‡")
            print(f"   CSVé¡ºåº: {list(df['material'])}")
            print(f"   æ ‡ç­¾é¡ºåº: {material_names}")
            continue

        embedding_columns = [f"emb_{i}" for i in range(EMBEDDING_DIM)]
        embeddings_np = df[embedding_columns].values
        embeddings_scaled_np, embedding_scaler = preprocess_embeddings(
            embeddings_np, method=EMBEDDING_PREPROCESSING_METHOD, verbose=True)

        X_embeddings = torch.tensor(embeddings_scaled_np, dtype=torch.float32)

        viz_dir = os.path.join(model_dir, "visualizations")
        os.makedirs(viz_dir, exist_ok=True)

        print(f"âœ… åŠ è½½å®Œæˆï¼Œå¼€å§‹æ‰§è¡ŒéªŒè¯æ–¹æ¡ˆ...")

        model_results = {}

        # === 1. çº¿æ€§æ¢é’ˆ (å…¨ä½“è®­ç»ƒ) ===
        linear_save_path = os.path.join(viz_dir, f"{model_dir}_linear_probe_predictions.csv")
        linear_mse, linear_table, _ = evaluate_probe_on_all_data(
            X_embeddings, labels_np, LinearProbe, material_names, PROPERTY_NAMES,
            save_path=linear_save_path,
            in_features=X_embeddings.shape[1], out_features=labels_np.shape[1]
        )
        model_results['linear_probe'] = {'mse': linear_mse, 'table': linear_table}

        # === 2. çº¿æ€§æ¢é’ˆ (ç•™ä¸€æ³•äº¤å‰éªŒè¯) ===
        linear_loo_save_path = os.path.join(viz_dir, f"{model_dir}_linear_probe_loo_cv.csv")
        linear_loo_metrics, linear_loo_prop_metrics, linear_loo_table, _ = evaluate_probe_loo_cv(
            X_embeddings, labels_np, LinearProbe, material_names, PROPERTY_NAMES,
            save_path=linear_loo_save_path,
            in_features=X_embeddings.shape[1], out_features=labels_np.shape[1]
        )
        model_results['linear_probe_loo'] = {
            'avg_metrics': linear_loo_metrics,
            'prop_metrics': linear_loo_prop_metrics,
            'table': linear_loo_table
        }

        # === æ–°å¢ï¼šä¸º LOO-CV ç»“æœç»˜å›¾ ===
        plot_loo_predictions(linear_loo_table, PROPERTY_NAMES, model_name, 'LinearProbe', viz_dir)

        # === 3. MLPæ¢é’ˆ (å…¨ä½“è®­ç»ƒ) ===
        mlp_save_path = os.path.join(viz_dir, f"{model_dir}_mlp_probe_predictions.csv")
        mlp_mse, mlp_table, _ = evaluate_probe_on_all_data(
            X_embeddings, labels_np, MLPProbe, material_names, PROPERTY_NAMES,
            save_path=mlp_save_path,
            in_features=X_embeddings.shape[1], out_features=labels_np.shape[1], hidden_dim=32
        )
        model_results['mlp_probe'] = {'mse': mlp_mse, 'table': mlp_table}

        # === 4. MLPæ¢é’ˆ (ç•™ä¸€æ³•äº¤å‰éªŒè¯) ===
        mlp_loo_save_path = os.path.join(viz_dir, f"{model_dir}_mlp_probe_loo_cv.csv")
        mlp_loo_metrics, mlp_loo_prop_metrics, mlp_loo_table, _ = evaluate_probe_loo_cv(
            X_embeddings, labels_np, MLPProbe, material_names, PROPERTY_NAMES,
            save_path=mlp_loo_save_path,
            in_features=X_embeddings.shape[1], out_features=labels_np.shape[1], hidden_dim=32
        )
        model_results['mlp_probe_loo'] = {
            'avg_metrics': mlp_loo_metrics,
            'prop_metrics': mlp_loo_prop_metrics,
            'table': mlp_loo_table
        }

        # === æ–°å¢ï¼šä¸º LOO-CV ç»“æœç»˜å›¾ ===
        plot_loo_predictions(mlp_loo_table, PROPERTY_NAMES, model_name, 'MLPProbe', viz_dir)

        # === 5. å¯è§†åŒ– t-SNE & PCA ===
        tsne_save_path = os.path.join(viz_dir, f"{model_dir}_embeddings_tsne.png")
        visualize_embeddings(embeddings_np, material_names, method='tsne', save_path=tsne_save_path)

        pca_save_path = os.path.join(viz_dir, f"{model_dir}_embeddings_pca.png")
        visualize_embeddings(embeddings_np, material_names, method='pca', save_path=pca_save_path)

        # === 6. ä½™å¼¦ç›¸ä¼¼åº¦çƒ­åŠ›å›¾ ===
        cosine_save_path = os.path.join(viz_dir, f"{model_dir}_cosine_similarity.png")
        plot_cosine_similarity_heatmap(embeddings_np, material_names, save_path=cosine_save_path)

        # === 7. èšç±»åˆ†æï¼ˆä»…ä¾›å‚è€ƒï¼‰===
        clustering_result = evaluate_clustering_metrics(embeddings_np, material_names)
        model_results['clustering'] = clustering_result

        # === æ–°å¢ï¼šç”Ÿæˆè¡¥å……çš„åˆ†æå›¾è¡¨ ===
        
        # 1.1 æ®‹å·®åˆ†å¸ƒ
        plot_residual_distribution(
            model_results['linear_probe_loo']['table'],
            model_results['mlp_probe_loo']['table'],
            PROPERTY_NAMES, model_name, viz_dir
        )
        
        # 1.2 è¯¯å·®ç®±çº¿å›¾
        plot_loo_error_boxplot(
            model_results['linear_probe_loo']['table'],
            model_results['mlp_probe_loo']['table'],
            PROPERTY_NAMES, model_name, viz_dir
        )
        
        # 2.2 åµŒå…¥ç»Ÿè®¡
        plot_embedding_statistics(embeddings_np, material_names, model_name, viz_dir)

        # 3.1 æ®‹å·® vs ç‰¹å¾
        plot_residuals_vs_feature(
            model_results['linear_probe_loo']['table'],
            model_results['mlp_probe_loo']['table'],
            labels_scaled_np_for_plotting,
            PROPERTY_NAMES, model_name, viz_dir, feature_idx=1  # ä½¿ç”¨å¸¦éš™ä½œä¸ºå…³é”®ç‰¹å¾
        )

        # ä¿å­˜åŸå§‹æ•°æ®å’Œå¤„ç†åçš„æ•°æ®
        data_summary_path = os.path.join(viz_dir, f"{model_dir}_data_summary.csv")
        
        summary_data = {
            'Material': material_names,
            'Embedding_Dim': [embeddings_np.shape[1]] * len(material_names)
        }

        # æ·»åŠ åŸå§‹æ ‡ç­¾
        for i, prop_name in enumerate(PROPERTY_NAMES):
            summary_data[f'Original_{prop_name}'] = labels_np[:, i]

        # æ·»åŠ æ ‡å‡†åŒ–åçš„æ ‡ç­¾
        for i, prop_name in enumerate(PROPERTY_NAMES):
            summary_data[f'Scaled_{prop_name}'] = labels_scaled_np_for_plotting[:, i]

        # æ·»åŠ åµŒå…¥å‘é‡ç»Ÿè®¡
        summary_data['Embedding_Mean'] = np.mean(embeddings_np, axis=1)
        summary_data['Embedding_Std'] = np.std(embeddings_np, axis=1)
        summary_data['Embedding_Min'] = np.min(embeddings_np, axis=1)
        summary_data['Embedding_Max'] = np.max(embeddings_np, axis=1)

        summary_df = pd.DataFrame(summary_data)
        summary_df.to_csv(data_summary_path, index=False, float_format='%.6f')
        print(f"   ğŸ’¾ æ•°æ®æ±‡æ€»å·²ä¿å­˜: {data_summary_path}")

        all_models_results[model_name] = model_results

        # ç”Ÿæˆç®€æ´æ€»ç»“
        print(f"\nğŸ“‹ {model_name} æ¼”ç¤ºç»“æœæ‘˜è¦:")
        print(f"   â”£â” Linear Probe (å…¨ä½“) MSE: {linear_mse:.4f}")
        print(f"   â”£â” Linear Probe (LOO-CV) MAE: {linear_loo_metrics['mae_avg']:.4f}")
        print(f"   â”£â” MLP Probe (å…¨ä½“) MSE: {mlp_mse:.4f}")
        print(f"   â”£â” MLP Probe (LOO-CV) MAE: {mlp_loo_metrics['mae_avg']:.4f}")
        print(f"   â”—â” è½®å»“åˆ†æ•° (ä»…ä¾›å‚è€ƒ): {clustering_result['best_silhouette']:.3f}")

    # æœ€ç»ˆæ±‡æ€»ï¼ˆç®€æ´ç‰ˆï¼‰
    if all_models_results:
        print(f"\n{'='*80}")
        print("ğŸ‰ æ‰€æœ‰æ¨¡å‹æ¼”ç¤ºå®Œæˆ â€”â€” æ¡†æ¶å¯è¡Œæ€§éªŒè¯æˆåŠŸï¼")
        print("ğŸ“š æ¨èå±•ç¤ºç´ æ:")
        print("   â€¢ æ¯ä¸ªæ¨¡å‹ visualizations/ ä¸‹çš„å›¾ (t-SNE, PCA, Cosine Heatmap, LOO-CVé¢„æµ‹æ•£ç‚¹å›¾, æ®‹å·®å›¾ç­‰)")
        print("   â€¢ æ§åˆ¶å°è¾“å‡ºçš„é¢„æµ‹å¯¹æ¯”è¡¨ï¼ˆå·²è‡ªåŠ¨ä¿å­˜ä¸ºCSVæ–‡ä»¶ï¼‰")
        print("   â€¢ å…¨ä½“è®­ç»ƒé¢„æµ‹æ•°æ®: {model_dir}_*_predictions.csv")
        print("   â€¢ ç•™ä¸€æ³•äº¤å‰éªŒè¯æ•°æ®: {model_dir}_*_loo_cv.csv")
        print("   â€¢ æ‰©å±•é¢„æµ‹æ•°æ®(åŒ…å«è¯¯å·®åˆ†æ): {model_dir}_*_predictions_detailed.csv")
        print("   â€¢ æ•°æ®æ±‡æ€»æ–‡ä»¶: {model_dir}_data_summary.csv")
        print("   â€¢ å„æ¨¡å‹MSE/MAEå¯¹æ¯”ï¼ˆå…¨ä½“è®­ç»ƒ vs LOO-CVï¼‰")
        print(f"{'='*80}")

        # è¾“å‡ºæ¨¡å‹æ’åº (å…¨ä½“è®­ç»ƒ)
        print("\nğŸ† æ¨¡å‹ç»¼åˆè¡¨ç°æ’åº (å…¨ä½“è®­ç»ƒå¹³å‡MSE â†“):")
        model_mses = []
        for name, res in all_models_results.items():
            avg_mse = (res['linear_probe']['mse'] + res['mlp_probe']['mse']) / 2
            model_mses.append((name, avg_mse))
        for name, mse in sorted(model_mses, key=lambda x: x[1]):
            print(f"   {name:<10} â†’ å¹³å‡MSE: {mse:.5f}")

        # è¾“å‡ºæ¨¡å‹æ’åº (LOO-CV)
        print("\nğŸ† æ¨¡å‹ç»¼åˆè¡¨ç°æ’åº (LOO-CVå¹³å‡MAE â†“):")
        model_maes = []
        for name, res in all_models_results.items():
            avg_mae = (res['linear_probe_loo']['avg_metrics']['mae_avg'] +
                      res['mlp_probe_loo']['avg_metrics']['mae_avg']) / 2
            model_maes.append((name, avg_mae))
        for name, mae in sorted(model_maes, key=lambda x: x[1]):
            print(f"   {name:<10} â†’ å¹³å‡MAE: {mae:.5f}")

        print(f"\nâœ… æ¼”ç¤ºç›®æ ‡è¾¾æˆï¼åµŒå…¥èƒ½é‡æ„æ€§è´¨ã€æ•è·ææ–™ç›¸ä¼¼æ€§ï¼Œæ¡†æ¶å¯è¡Œã€‚")
        print(f"ğŸ’¡ LOO-CVç»“æœæ›´èƒ½åæ˜ æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œæ˜¯è¯„ä¼°åµŒå…¥è´¨é‡çš„å…³é”®æŒ‡æ ‡ï¼")

if __name__ == "__main__":
    main()
